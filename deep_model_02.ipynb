{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data1.avi', 'data2.avi', 'data3.avi', 'data4.avi', 'data5.avi', 'data6.avi', 'data7.avi', 'data8.avi', 'data9.avi', 'data10.avi', 'data11.avi', 'data12.avi', 'data13.avi', 'data14.avi', 'data15.avi', 'data16.avi', 'data17.avi', 'data18.avi', 'data19.avi', 'data20.avi', 'data21.avi', 'data22.avi', 'data23.avi', 'data24.avi', 'data25.avi', 'data26.avi', 'data27.avi', 'data29.avi', 'data30.avi', 'data31.avi', 'data32.avi', 'data33.avi', 'data34.avi', 'data35.avi', 'data36.avi', 'data37.avi', 'data38.avi', 'data39.avi', 'data40.avi']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "real_index = []\n",
    "for i in range (1,41):\n",
    "    j= str(i)\n",
    "    index = 'data'+ j +'.avi'\n",
    "    # print(index)\n",
    "    real_index.append(index)\n",
    "\n",
    "# print(real_index)\n",
    "real_index.remove('data28.avi')\n",
    "print(real_index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# imports\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy\n",
    "\n",
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1.avi\n",
      "(1205, 256, 6, 18)\n",
      "data2.avi\n",
      "(1208, 256, 7, 20)\n",
      "data3.avi\n",
      "(1207, 256, 8, 23)\n",
      "data4.avi\n",
      "(1209, 256, 7, 20)\n",
      "data5.avi\n",
      "(1210, 256, 7, 21)\n",
      "data6.avi\n",
      "(1208, 256, 8, 22)\n",
      "data7.avi\n",
      "(1209, 256, 7, 19)\n",
      "data8.avi\n",
      "(1210, 256, 8, 23)\n",
      "data9.avi\n",
      "(1225, 256, 7, 20)\n",
      "data10.avi\n",
      "(1227, 256, 8, 23)\n",
      "data11.avi\n",
      "(1226, 256, 6, 18)\n",
      "data12.avi\n",
      "(1577, 256, 6, 18)\n",
      "data13.avi\n",
      "(1221, 256, 8, 23)\n",
      "data14.avi\n",
      "(1248, 256, 6, 18)\n",
      "data15.avi\n",
      "(1298, 256, 7, 19)\n",
      "data16.avi\n",
      "(1232, 256, 8, 22)\n",
      "data17.avi\n",
      "(1226, 256, 7, 21)\n",
      "data18.avi\n",
      "(1243, 256, 8, 24)\n",
      "data19.avi\n",
      "(1209, 256, 7, 19)\n",
      "data20.avi\n",
      "(1209, 256, 9, 25)\n",
      "data21.avi\n",
      "(1208, 256, 7, 20)\n",
      "data22.avi\n",
      "(1209, 256, 7, 21)\n",
      "data23.avi\n",
      "(1208, 256, 7, 21)\n",
      "data24.avi\n",
      "(1209, 256, 9, 26)\n",
      "data25.avi\n",
      "(1233, 256, 7, 21)\n",
      "data26.avi\n",
      "(1230, 256, 7, 21)\n",
      "data27.avi\n",
      "(1227, 256, 6, 18)\n",
      "data29.avi\n",
      "(2154, 256, 8, 22)\n",
      "data30.avi\n",
      "(1231, 256, 7, 22)\n",
      "data31.avi\n",
      "(1245, 256, 6, 18)\n",
      "data32.avi\n",
      "(1217, 256, 7, 19)\n",
      "data33.avi\n",
      "(1216, 256, 7, 19)\n",
      "data34.avi\n",
      "(1224, 256, 7, 20)\n",
      "data35.avi\n",
      "(1235, 256, 9, 26)\n",
      "data36.avi\n",
      "(1233, 256, 7, 20)\n",
      "data37.avi\n",
      "(1209, 256, 8, 23)\n",
      "data38.avi\n",
      "(1233, 256, 8, 24)\n",
      "data39.avi\n",
      "(1223, 256, 7, 21)\n",
      "data40.avi\n",
      "(1250, 256, 7, 21)\n"
     ]
    }
   ],
   "source": [
    "All_features = []\n",
    "for k in real_index:\n",
    "    print(k)\n",
    "    # Step 1: Load the video file and extract each frame\n",
    "    cap = cv2.VideoCapture(k)\n",
    "\n",
    "    # Step 2: Initialize MediaPipe's face detection model\n",
    "    mp_face_detection = mp.solutions.face_detection\n",
    "    face_detection = mp_face_detection.FaceDetection()\n",
    "\n",
    "    # Step 3: Detect faces in each frame and select ROI for each face\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the image to RGB format for MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detect faces using MediaPipe's face detection model\n",
    "        results = face_detection.process(frame_rgb)\n",
    "\n",
    "        # Iterate through each detected face and select ROI\n",
    "        for detection in results.detections:\n",
    "            bbox = detection.location_data.relative_bounding_box\n",
    "            h, w, _ = frame.shape\n",
    "            x, y, w, h = int(bbox.xmin * w), int(bbox.ymin * h), int(bbox.width * w), int(bbox.height * h)\n",
    "\n",
    "            # Select ROI for each face (forehead, cheek, and nose)\n",
    "            forehead_roi = frame[y:y+h//3, x:x+w]\n",
    "            cheek_roi = frame[y+h//3:y+2*h//3, x:x+w]\n",
    "            nose_roi = frame[y+2*h//3:y+h, x:x+w]\n",
    "\n",
    "            # Draw bounding box and ROI on the frame\n",
    "            # cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h//3), (255, 0, 0), 2) # Forehead ROI\n",
    "            cv2.rectangle(frame, (x, y+h//3), (x+w, y+2*h//3), (0, 0, 255), 2) # Cheek ROI\n",
    "            cv2.rectangle(frame, (x, y+2*h//3), (x+w, y+h), (255, 255, 0), 2) # Nose ROI\n",
    "\n",
    "        # Show the frame with detected face area and ROI area\n",
    "        # cv2.imshow('frame', frame)\n",
    "        # if cv2.waitKey(1) == ord('q'):\n",
    "        #     break\n",
    "\n",
    "    # Release the video capture and destroy all windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "#################################################\n",
    "    # using Resnet 18\n",
    "    resnet18 = torchvision.models.resnet18()\n",
    "    modules = list(resnet18.children())[: -4]   # Select up to the 14th layer\n",
    "    model = torch.nn.Sequential(*modules)\n",
    "\n",
    "    # Step 2: Normalize the pixel values in each map\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # Step 3: Extract spatial-temporal features from ROIs in each frame\n",
    "    feature_maps = []\n",
    "    cap = cv2.VideoCapture(k)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the ROIs to PyTorch tensor and normalize the pixel values\n",
    "        forehead_tensor = normalize(transforms.ToTensor()(forehead_roi)).unsqueeze(0)\n",
    "        cheek_tensor = normalize(transforms.ToTensor()(cheek_roi)).unsqueeze(0)\n",
    "        # nose_tensor = normalize(transforms.ToTensor()(nose_roi)).unsqueeze(0)\n",
    "\n",
    "        # Extract features using the pre-trained ResNet18 model\n",
    "        with torch.no_grad():\n",
    "            forehead_features = model(forehead_tensor).squeeze().numpy()\n",
    "            cheek_features = model(cheek_tensor).squeeze().numpy()\n",
    "            # nose_features = model(nose_tensor).squeeze().numpy()\n",
    "\n",
    "        # Step 2: Normalize the pixel values in each map to have a mean of zero and a standard deviation of one\n",
    "        forehead_features = (forehead_features - np.mean(forehead_features)) / np.std(forehead_features)\n",
    "        cheek_features = (cheek_features - np.mean(cheek_features)) / np.std(cheek_features)\n",
    "        # nose_features = (nose_features - np.mean(nose_features)) / np.std(nose_features)\n",
    "\n",
    "        # Step 3: Create a spatial-temporal map by combining the normalized maps\n",
    "        spatial_temporal_map = np.concatenate((forehead_features, cheek_features), axis=0)\n",
    "        feature_maps.append(spatial_temporal_map)\n",
    "\n",
    "    # Convert the feature maps to a NumPy array\n",
    "    feature_maps = np.array(feature_maps)\n",
    "\n",
    "    # Release the video capture and destroy all windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "#################################################\n",
    "    print(feature_maps.shape)\n",
    "    All_features.append(feature_maps)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "(1200, 256, 6, 18)\n"
     ]
    }
   ],
   "source": [
    "# All_features = torch.tensor(All_features)\n",
    "All_feature_size = []\n",
    "for i in All_features:\n",
    "    a = i[0:1200 , : , 0:6, 0:18 ]\n",
    "    All_feature_size.append(a)\n",
    "    # print(a.shape) #1200*128\n",
    "\n",
    "\n",
    "print(len(All_feature_size))\n",
    "print(All_feature_size[0].shape)\n",
    "All_feature_size = torch.tensor(All_feature_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n",
      "torch.Size([1200, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "All_flattened = []\n",
    "num_channels = 256 #(num_frames, num_channels, height, width)\n",
    "\n",
    "for i in All_feature_size:\n",
    "\n",
    "    class ConvNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(ConvNet, self).__init__()\n",
    "\n",
    "            self.conv1 = nn.Conv2d(in_channels=num_channels, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "            self.flatten = nn.Flatten()\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = nn.functional.relu(x)\n",
    "            x = self.pool1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = nn.functional.relu(x)\n",
    "            x = self.pool2(x)\n",
    "            x = self.flatten(x)\n",
    "\n",
    "            flattened_features = x\n",
    "\n",
    "            return flattened_features\n",
    "\n",
    "    conv_net = ConvNet()\n",
    "    stacked_tensor = i\n",
    "    flattened_features = conv_net(stacked_tensor)\n",
    "    print(flattened_features.shape)\n",
    "    All_flattened.append(flattened_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "torch.Size([39])\n"
     ]
    }
   ],
   "source": [
    "#39 members\n",
    "HR = [84,71,70,76,73,63,61,72,75,84,67,73,68,86,91,60,64,81,69,74,83,61,83,95,90,88,66,96,58,56,61,76,62,88,92,63,64,60,64]\n",
    "print(len(HR))\n",
    "\n",
    "HR_tensor = torch.tensor(HR)\n",
    "print(HR_tensor.shape)\n",
    "HR_tensor_float = HR_tensor.float()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 1200, 128])\n",
      "torch.Size([39, 1200, 128])\n"
     ]
    }
   ],
   "source": [
    "data_3d = torch.stack(All_flattened) #(num_samples or batches, num_frames, flattened_feature_size)\n",
    "print(data_3d.shape)\n",
    "\n",
    "data_3d_detach = data_3d.detach()\n",
    "print(data_3d_detach.shape)  #(num_samples, num_frames, flattened_feature_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mojta\\anaconda3\\envs\\cudatest\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Train Loss: 4864.2042\n",
      "Epoch [2/30], Train Loss: 3556.1870\n",
      "Epoch [3/30], Train Loss: 2743.9351\n",
      "Epoch [4/30], Train Loss: 2110.7033\n",
      "Epoch [5/30], Train Loss: 1613.3036\n",
      "Epoch [6/30], Train Loss: 1222.3968\n",
      "Epoch [7/30], Train Loss: 914.7870\n",
      "Epoch [8/30], Train Loss: 681.9811\n",
      "Epoch [9/30], Train Loss: 509.8370\n",
      "Epoch [10/30], Train Loss: 384.4843\n",
      "Epoch [11/30], Train Loss: 294.8629\n",
      "Epoch [12/30], Train Loss: 232.0637\n",
      "Epoch [13/30], Train Loss: 188.9836\n",
      "Epoch [14/30], Train Loss: 160.0762\n",
      "Epoch [15/30], Train Loss: 141.1175\n",
      "Epoch [16/30], Train Loss: 128.9764\n",
      "Epoch [17/30], Train Loss: 121.3942\n",
      "Epoch [18/30], Train Loss: 116.7857\n",
      "Epoch [19/30], Train Loss: 114.0685\n",
      "Epoch [20/30], Train Loss: 112.5229\n",
      "Epoch [21/30], Train Loss: 111.6831\n",
      "Epoch [22/30], Train Loss: 111.2557\n",
      "Epoch [23/30], Train Loss: 111.0608\n",
      "Epoch [24/30], Train Loss: 110.9915\n",
      "Epoch [25/30], Train Loss: 110.9859\n",
      "Epoch [26/30], Train Loss: 111.0097\n",
      "Epoch [27/30], Train Loss: 111.0447\n",
      "Epoch [28/30], Train Loss: 111.0819\n",
      "Epoch [29/30], Train Loss: 111.1173\n",
      "Epoch [30/30], Train Loss: 111.1493\n",
      "Test Loss: 201.7960, MAE: 13.3589, MAPE: 20.7027\n"
     ]
    }
   ],
   "source": [
    "# LSTM MODEL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# set random seed for reproducibility\n",
    "# torch.manual_seed(42)\n",
    "\n",
    "# define the LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# define the hyperparameters\n",
    "input_size = 128   #flattened_feature_size\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "output_size = 1\n",
    "lr = 0.001\n",
    "num_epochs = 30\n",
    "batch_size = 1\n",
    "\n",
    "# split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data_3d_detach, HR_tensor_float, test_size=0.3, random_state=42)\n",
    "# Split the data into train and test sets\n",
    "num_samples = 40\n",
    "train_size = int(0.7 * num_samples)\n",
    "test_size = num_samples - train_size\n",
    "data_train = data_3d_detach[:train_size]\n",
    "data_test = data_3d_detach[train_size:]\n",
    "HR_train = HR_tensor_float[:train_size]\n",
    "HR_test = HR_tensor_float[train_size:]\n",
    "\n",
    "# convert the data to PyTorch tensors\n",
    "X_train = data_train\n",
    "y_train = HR_train\n",
    "X_test = data_test\n",
    "y_test = HR_test\n",
    "\n",
    "# create data loaders for batch processing\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# create the model and optimizer\n",
    "model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# criterion = nn.L1Loss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print(\"Epoch [{}/{}], Train Loss: {:.4f}\".format(epoch+1, num_epochs, train_loss/len(train_loader)))\n",
    "\n",
    "# evaluate the model\n",
    "test_loss = 0.0\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        predictions.extend(outputs.detach().numpy())\n",
    "mae = np.mean(np.abs(np.array(predictions) - y_test.numpy()))\n",
    "mape = np.mean(np.abs(np.array(predictions) - y_test.numpy()) / np.array(y_test.numpy()))*100\n",
    "\n",
    "print(\"Test Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}\".format(test_loss/len(test_loader), mae, mape))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
